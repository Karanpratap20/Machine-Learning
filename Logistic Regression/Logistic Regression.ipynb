{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "#Importing the dataset and dropping the unwanted columns \n",
    "heart_df=pd.read_csv(\"./framingham.csv\")\n",
    "heart_df.drop(['education'],axis=1,inplace=True)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.rename(columns={'male':'Sex_male'},inplace=True)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the missing values \n",
    "heart_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the missing values and dropping them\n",
    "count=0\n",
    "for i in heart_df.isnull().sum(axis=1):\n",
    "    if i>0:\n",
    "        count=count+1\n",
    "print('Total number of rows with missing values is ', count)\n",
    "print('since it is only',round((count/len(heart_df.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')\n",
    "\n",
    "heart_df.dropna(axis=0,inplace=True)\n",
    "\n",
    "heart_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a constant\n",
    "from statsmodels.tools import add_constant as add_constant\n",
    "heart_df_constant = add_constant(heart_df)\n",
    "heart_df_constant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.chisqprob = lambda chisq, df: st.chi2.sf(chisq, df)\n",
    "cols=heart_df_constant.columns[:-1]\n",
    "model=sm.Logit(heart_df.TenYearCHD,heart_df_constant[cols])\n",
    "result=model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_feature_elem (data_frame,dep_var,col_list):\n",
    "    while len(col_list)>0 :\n",
    "        model=sm.Logit(dep_var,data_frame[col_list])\n",
    "        result=model.fit(disp=0)\n",
    "        largest_pvalue=round(result.pvalues,3).nlargest(1)\n",
    "        if largest_pvalue[0]<(0.05):\n",
    "            return result\n",
    "            break\n",
    "        else:\n",
    "            col_list=col_list.drop(largest_pvalue.index)\n",
    "\n",
    "result=back_feature_elem(heart_df_constant,heart_df.TenYearCHD,cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpreting the results: Odds Ratio, Confidence Intervals and Pvalues\n",
    "\n",
    "params = np.exp(result.params)\n",
    "conf = np.exp(result.conf_int())\n",
    "conf['OR'] = params\n",
    "pvalue=round(result.pvalues,3)\n",
    "conf['pvalue']=pvalue\n",
    "conf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\n",
    "print ((conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "new_features=heart_df[['age','Sex_male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]\n",
    "x=new_features.iloc[:,:-1]\n",
    "y=new_features.iloc[:,-1]\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred=logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "confusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
